{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Optimizing a basic model with a Built Algorithm\n",
    "\n",
    "This exercise is about executing all the steps of the Machine Learning development pipeline, using some features SageMaker offers. We'll use here a public dataset called iris. The dataset and the model aren't the focus of this exercise. The idea here is to see how SageMaker can accelerate your work and avoid wasting your time with tasks that aren't related to your business. So, we'll do the following:\n",
    "\n",
    " - **PART 1** - Train/deploy/test a multiclass model using XGBoost\n",
    "     - *Monitoring*:\n",
    "         - We will generate a **baseline** for the monitoring. Yes, we can monitor a deployed model by collecting logs from the payload and the model output. SageMaker can suggest some statistics and constraints that can be used to compare with the collected data. Then we can see some **metrics** related to the **model performance**.\n",
    "         - We'll also create a monitoring scheduler. With this scheduler, SageMaker will parse the logs from time to time to compute the metrics we need. Given it takes some time to get the results, we'll check these metrics at the end of the exercice, in **Part 4**.\n",
    " - **PART 2** - Optimize the model\n",
    " - **PART 3** - Run batch predictions\n",
    " - **PART 4** - Check the monitoring results, created in **Part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - Train deploy and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's  start by importing the dataset and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "dataset = np.insert(iris.data, 0, iris.target,axis=1)\n",
    "\n",
    "df = pd.DataFrame(data=dataset, columns=['iris_id'] + iris.feature_names)\n",
    "## We'll also save the dataset, with header, give we'll need to create a baseline for the monitoring\n",
    "df.to_csv('full_dataset.csv', sep=',', index=None)\n",
    "df['species'] = df['iris_id'].map(lambda x: 'setosa' if x == 0 else 'versicolor' if x == 1 else 'virginica')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby(df['species'])['species'].count().plot(kind='bar')\n",
    "x_offset = -0.05\n",
    "y_offset = 0\n",
    "for p in ax.patches:\n",
    "    b = p.get_bbox()\n",
    "    val = \"{}\".format(int(b.y1 + b.y0))\n",
    "    ax.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\"f\",\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "            ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplots & histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.drop(['iris_id'], axis=1), hue='species', size=2.5,diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.drop(['iris_id'], axis=1), kind=\"reg\", hue='species', size=2.5,diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a plot a kernel density estimate.\n",
    "We can see in this dimension an overlaping between **versicolor** and **virginica**. This is a better representation of what we identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df[(df.iris_id==0.0)]\n",
    "sns.kdeplot(tmp_df['petal width (cm)'], tmp_df['petal length (cm)'], bw='silverman', \n",
    "            cmap=\"Blues\", shade=False, shade_lowest=False)\n",
    "\n",
    "tmp_df = df[(df.iris_id==1.0)]\n",
    "sns.kdeplot(tmp_df['petal width (cm)'], tmp_df['petal length (cm)'], bw='silverman', \n",
    "            cmap=\"Greens\", shade=False, shade_lowest=False)\n",
    "\n",
    "tmp_df = df[(df.iris_id==2.0)]\n",
    "sns.kdeplot(tmp_df['petal width (cm)'], tmp_df['petal length (cm)'], bw='silverman',\n",
    "            cmap=\"Reds\", shade=False, shade_lowest=False)\n",
    "\n",
    "plt.xlabel('species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Petal length and petal width have the highest linear correlation with our label. Also, sepal width seems to be useless, considering the linear correlation with our label.\n",
    "\n",
    "Since versicolor and virginica cannot be split linearly, we need a more versatile algorithm to create a better classifier. In this case, we'll use XGBoost, a tree ensable that can give us a good model for predicting the flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now let's split the dataset into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "df = pd.DataFrame(data=X_test)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('iris_train.csv', 'w') as csv:\n",
    "    for x_,y_ in zip(X_train, y_train):\n",
    "        line = \"%s,%s\" % (y_, \",\".join( list(map(str, x_)) ) )\n",
    "        csv.write( line + \"\\n\" )\n",
    "    csv.flush()\n",
    "    csv.close()\n",
    "\n",
    "with open('iris_test.csv', 'w') as csv:\n",
    "    for x_,y_ in zip(X_test, y_test):\n",
    "        line = \"%s,%s\" % (y_, \",\".join( list(map(str, x_)) ) )\n",
    "        csv.write( line + \"\\n\" )\n",
    "    csv.flush()\n",
    "    csv.close()\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's time to train our model with the builtin algorithm XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "prefix='mlops/iris'\n",
    "# Retrieve the default bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will launch an async job to create the baseline for the monitoring process\n",
    "A baseline is a what the monitoring will consider **normal**. \n",
    "\n",
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should exactly match (i.e. the number and order of the features).\n",
    "\n",
    "From the training dataset you can ask Amazon SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data. For this example, upload the training dataset that was used to train the pre-trained model included in this example. If you already have it in Amazon S3, you can directly point to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "endpoint_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "endpoint_monitor.suggest_baseline(\n",
    "    baseline_dataset='full_dataset.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri='s3://{}/{}/monitoring/baseline'.format(bucket, prefix),\n",
    "    wait=False,\n",
    "    logs=False\n",
    ")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok. Let's continue, upload the dataset and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to an S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='iris_train.csv', key_prefix='%s/data' % prefix)\n",
    "input_test = sagemaker_session.upload_data(path='iris_test.csv', key_prefix='%s/data' % prefix)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_data=input_train,content_type=\"csv\")\n",
    "test_data = sagemaker.session.s3_input(s3_data=input_test,content_type=\"csv\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the URI for new container\n",
    "container_uri = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version='0.90-2'); \n",
    "\n",
    "# Create the estimator\n",
    "xgb = sagemaker.estimator.Estimator(container_uri,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "# Set the hyperparameters\n",
    "xgb.set_hyperparameters(eta=0.1,\n",
    "                        max_depth=10,\n",
    "                        gamma=4,\n",
    "                        num_class=len(np.unique(y)),\n",
    "                        alpha=10,\n",
    "                        min_child_weight=6,\n",
    "                        silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_round=30)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes around 3min 11s\n",
    "xgb.fit({'train': train_data, 'validation': test_data, })\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model and create an endpoint for it\n",
    "The following action will:\n",
    " * get the assets from the job we just ran and then create an input in the Models Catalog\n",
    " * create a endpoint configuration (a metadata for our final endpoint)\n",
    " * create an enpoint, which is our model wrapped in a format of a WebService\n",
    " \n",
    "After that we'll be able to call our deployed endpoint for doing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Enable log capturing in the endpoint\n",
    "data_capture_configuration = sagemaker.model_monitor.data_capture_config.DataCaptureConfig(\n",
    "    enable_capture=True, \n",
    "    sampling_percentage=100, \n",
    "    destination_s3_uri='s3://{}/{}/monitoring'.format(bucket, prefix), \n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge',\n",
    "    data_capture_config=data_capture_configuration\n",
    ")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, now that we have deployed the endpoint, with data capturing enabled, it's time to setup the monitor\n",
    "Let's start by configuring our predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "endpoint_name = xgb_predictor.endpoint\n",
    "model_name = boto3.client('sagemaker').describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name\n",
    ")['ProductionVariants'][0]['ModelName']\n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we need to create a **Monitoring Schedule** for our endpoint. The command below will create a cron scheduler that will process the log each hour, then we can see how well our model is going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_monitor.create_monitoring_schedule(\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri='s3://{}/{}/monitoring/reports'.format(bucket, prefix),\n",
    "    statistics=endpoint_monitor.baseline_statistics(),\n",
    "    constraints=endpoint_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just take a look on the baseline created/sugested by SageMaker for your dataset\n",
    "This set of statistics and constraints will be used by the Monitoring Scheduler to compare the incoming data with what is considered **normal**. Each invalid payload sent to the endpoint will be considered a violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = endpoint_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "report_df = schema_df.merge(constraints_df)\n",
    "report_df.drop([\n",
    "    'numerical_statistics.distribution.kll.buckets',\n",
    "    'numerical_statistics.distribution.kll.sketch.data',\n",
    "    'numerical_statistics.distribution.kll.sketch.parameters.c'\n",
    "], axis=1).head(10)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start generating some artificial traffic\n",
    "The cell below starts a thread to send some traffic to the endpoint. Note that you need to stop the kernel to terminate this thread. If there is no traffic, the monitoring jobs are marked as `Failed` since there is no data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time \n",
    "from threading import Thread\n",
    "\n",
    "traffic_generator_running=False\n",
    "def invoke_endpoint_forever():\n",
    "    print('Invoking endpoint forever!')\n",
    "    while traffic_generator_running:\n",
    "        ## This will create an invalid set of features\n",
    "        ## The idea is to violate two monitoring constraings: not_null and data_drift\n",
    "        null_idx = random.randint(0,3)\n",
    "        sample = [random.randint(500,2000) / 100.0 for i in range(4)]\n",
    "        sample[null_idx] = None\n",
    "        xgb_predictor.predict(sample)\n",
    "        time.sleep(0.5)\n",
    "    print('Endpoint invoker has stopped')\n",
    "Thread(target = invoke_endpoint_forever).start()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's do a basic test with the deployed endpoint\n",
    "In this test, we'll use a helper object called predictor. This object is always returned from a **Deploy** call. The predictor is just for testing purposes and we'll not use it inside our real application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = [ float(xgb_predictor.predict(x).decode('utf-8')) for x in X_test] \n",
    "score = f1_score(y_test,predictions_test,labels=[0.0,1.0,2.0],average='micro')\n",
    "\n",
    "print('F1 Score(micro): %.1f' % (score * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, let's  test the API for our trained model\n",
    "This is how your application will call the endpoint. Using boto3 for getting a sagemaker runtime client and then we'll call invoke_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "sm = boto3.client('sagemaker-runtime')\n",
    "resp = sm.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='text/csv',\n",
    "    Body=csv_serializer(X_test[0])\n",
    ")\n",
    "prediction = float(resp['Body'].read().decode('utf-8'))\n",
    "print('Predicted class: %.1f for [%s]' % (prediction, csv_serializer(X_test[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - Model optimization with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Jobs\n",
    "#### A.K.A. Hyperparameter Optimization\n",
    "\n",
    "## Let's tune our model before using it for our batch prediction\n",
    "We know that the iris dataset is an easy challenge. We can achieve a better score with XGBoost. However, we don't want to wast time testing all the possible variations of the hyperparameters in order to optimize the training process.\n",
    "\n",
    "Instead, we'll use the Sagemaker's tuning feature. For that, we'll use the same estimator, but let's create a Tuner and ask it for optimize the model for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                         'gamma': ContinuousParameter(0, 10),\n",
    "                        'max_depth': IntegerParameter(1, 10)}\n",
    "\n",
    "objective_metric_name = 'validation:merror'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=20,\n",
    "                            max_parallel_jobs=4,\n",
    "                            objective_type='Minimize')\n",
    "\n",
    "tuner.fit({'train': train_data, 'validation': test_data, })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = tuner.latest_tuning_job.name\n",
    "attached_tuner = HyperparameterTuner.attach(job_name)\n",
    "xgb_predictor2 = attached_tuner.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_endpoint_name = endpoint_name\n",
    "endpoint_name = xgb_predictor2.endpoint\n",
    "model_name = boto3.client('sagemaker').describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name\n",
    ")['ProductionVariants'][0]['ModelName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple test before we move on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "xgb_predictor2.content_type = 'text/csv'\n",
    "xgb_predictor2.serializer = csv_serializer\n",
    "xgb_predictor2.deserializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = [ float(xgb_predictor2.predict(x).decode('utf-8')) for x in X_test] \n",
    "score = f1_score(y_test,predictions_test,labels=[0.0,1.0,2.0],average='micro')\n",
    "\n",
    "print('F1 Score(micro): %.1f' % (score * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3 - Batch Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform job\n",
    "If you have a file with the samples you want to predict, just upload that file to an S3 bucket and start a Batch Transform job. For this task, you don't need to deploy an endpoint. Sagemaker will create all the resources needed to do this batch prediction, save the results into an S3 bucket and then it will destroy the resources automatically for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dataset_filename='batch_dataset.csv'\n",
    "with open(batch_dataset_filename, 'w') as csv:\n",
    "    for x_ in X:\n",
    "        line = \",\".join( list(map(str, x_)) )\n",
    "        csv.write( line + \"\\n\" )\n",
    "    csv.flush()\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = sagemaker_session.upload_data(path=batch_dataset_filename, key_prefix='%s/data' % prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Initialize the transformer object\n",
    "transformer=sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name='mlops-iris',\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    output_path='s3://{}/{}/batch_output'.format(bucket, prefix),\n",
    ")\n",
    "# To start a transform job:\n",
    "transformer.transform(input_batch, content_type='text/csv', split_type='Line')\n",
    "# Then wait until transform job is completed\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "predictions_filename='iris_predictions.csv'\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket, '{}/batch_output/{}.out'.format(prefix, batch_dataset_filename), predictions_filename)\n",
    "\n",
    "df2 = pd.read_csv(predictions_filename, sep=',', encoding='utf-8',header=None, names=[ 'predicted_iris_id'])\n",
    "df3 = df.copy()\n",
    "df3['predicted_iris_id'] = df2['predicted_iris_id']\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "score = f1_score(df3['iris_id'], df3['predicted_iris_id'],labels=[0.0,1.0,2.0],average='micro')\n",
    "\n",
    "print('F1 Score(micro): %.1f' % (score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(df3['iris_id'], df3['predicted_iris_id'])\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.heatmap(cnf_matrix, annot=True, fmt=\"f\", mask=np.zeros_like(cnf_matrix, dtype=np.bool), \n",
    "            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4 - Checking the monitoring results\n",
    "The HPO took something like 20 minutes to run. The batch prediction, 3-5 more. It is probably enough time to have at least one execution of the monitor schedule. Since we created a thread for generating **invalid** features, we must have some data drift detected in our monitoring. Let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = endpoint_monitor.list_executions()\n",
    "print(\"We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = endpoint_monitor.list_executions()\n",
    "print('OK. we have %d execution(s) now' % len(mon_executions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mon_executions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dd59df96502b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_executions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ProcessingJobStatus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExitMessage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mon_executions' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_constraint_violations():\n",
    "    violations = endpoint_monitor.latest_monitoring_constraint_violations()\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "    display(HTML(constraints_df.head(10).to_html()))\n",
    "    \n",
    "while True:\n",
    "    resp = mon_executions[-1].describe()\n",
    "    status = resp['ProcessingJobStatus']\n",
    "    msg = resp['ExitMessage']\n",
    "    if status == 'InProgress':\n",
    "        time.sleep(30)\n",
    "    elif status == 'Completed':\n",
    "        print(\"Finished: %s\" % msg)\n",
    "        print_constraint_violations()\n",
    "        break\n",
    "    else:\n",
    "        print(\"Error: %s\" % msg)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check these metrics on CloudWatch. Just open the CloudWatch console, click on **Metrics**, then select:\n",
    "    All -> aws/sagemaker/Endpoints/data-metrics -> Endpoint, MonitoringSchedule\n",
    "\n",
    "Use the *endpoint_monitor* name to filter the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'endpoint_monitor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-738b9dfa8920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraffic_generator_running\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#time.sleep(3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mendpoint_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_monitoring_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# wait for 10 seconds before trying to delete the endpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'endpoint_monitor' is not defined"
     ]
    }
   ],
   "source": [
    "traffic_generator_running=False\n",
    "time.sleep(3)\n",
    "endpoint_monitor.delete_monitoring_schedule()\n",
    "time.sleep(10) # wait for 10 seconds before trying to delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5cbdd680d8bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxgb_predictor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_predictor.delete_endpoint()\n",
    "xgb_predictor2.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
